# Data Pipeline Project

A robust, containerized data pipeline built with Python, Apache Airflow, PostgreSQL, and MongoDB. This project demonstrates how to:

- **Ingest Data:** Fetch data from an external API using Python.
- **Clean and Transform:** Process and normalize the raw data.
- **Store Data:** Insert the cleaned data into both a PostgreSQL (SQL) and a MongoDB (NoSQL) database.
- **Orchestrate Workflows:** Automate and monitor the entire process using an Airflow DAG.

All services are containerized using Docker Compose for a reproducible and isolated development environment. Whether you're a data engineer or developer, this project offers a practical example of building and managing a modern data pipeline.